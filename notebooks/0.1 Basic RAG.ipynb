{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261eadc9ca7562d4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction\n",
    "For the simple demo of RAG pipeline, lets build a simple chatbot that can answer questions from PDF. By the end of this section, you will have a good overview of how to build a simple RAG chatbot using Langchain and Python. For this exercise, We will be using OpenAI models for this so please keep the OPENAI_API_KEY handy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248deb448a53788d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Ingestion:\n",
    "\n",
    "For this exercise, we will be using a couple of text files for the demonstration purpose.Those files are available in the \"data\" folder.   \n",
    "\n",
    "Lets start! \n",
    "\n",
    "#### Install Necessary Libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32968d098c54489",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from openai._compat import model_json\n",
    "!pip install langchain langchain_community langchain_openai chromadb langchainhub umap-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb47b15dc83c897b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup the necessary Environment Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926cb83c669d880b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['OPENAI_MODEL_NAME'] = 'gpt-4-1106-preview'\n",
    "# os.environ['OPENAI_API_KEY'] = 'sk-XXXXXXXXXXX'\n",
    "# os.environ['OPENAI_API_BASE'] = 'https://api.openai.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2a84be1422be0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Better way to manage Environment Variables:\n",
    "\n",
    "Keep these env variables in .env file for better management. You can use `python-dotenv` library to load the .env file.\n",
    ".env file should be in the root directory of the project. Following is the example of .env file:\n",
    "\n",
    "```shell\n",
    "OPENAI_MODEL_NAME=gpt-4-1106-preview\n",
    "OPENAI_API_KEY=sk-XXXXXXXXXXX\n",
    "OPENAI_API_BASE=https://api.openai.com\n",
    "```\n",
    "\n",
    "load these env variables using the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd188cf78122c0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())  # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5df06ad770c42",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load the Text Data:\n",
    "\n",
    "The first steps is to load text data into memory. We will use DirectoryLoader to load text files from the data folder. The `DirectoryLoader` class is used to load text files from a directory. The `TextLoader` class is used to load text files. The `load` method is used to load the text files into memory. The `page_content` attribute is used to access the text content of the loaded documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9787553668ea165",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.document_loaders.directory import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader('../data', glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "documents[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ce431c33e2679c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Split the Text into small chunks:\n",
    "\n",
    "This segment breaks the text into smaller pieces or chunks using `RecursiveCharacterTextSplitter`. This is helpful for processing large documents in manageable parts. The `chunk_size` parameter defines the maximum size of each chunk, while `chunk_overlap` allows for some overlap between consecutive chunks to ensure continuity in the context. `len(docs)` shows the total number of chunks created. This is one of the most popular way to create chunks. We will discuss more ways in subsequent articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d1596d81037e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0350e213090cc65",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create Embeddings\n",
    "\n",
    "Next, we convert the split documents into embeddings using `OpenAIEmbeddings` and stores these embeddings in a vector store (`Chroma`). Embeddings are vector representations of text, useful for various NLP tasks. This process is essential for creating a searchable database of text chunks based on their semantic content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28193617c14b5eb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284b957b33b3aee0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Until this point in time, its a onetime process to create embeddings and store them in the vector store. You can use this vector store for any downstream tasks like Q&A, Chatbot, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e44920a01bf5c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Retrieval:\n",
    "\n",
    "Now, since, we have the data and database ready, we can start building retrieval and generation part. So here, whenever user asks a question, we will do following\n",
    "1. Convert the question into embedding\n",
    "2. Gather approximate nearest neighbours of the query embedding from database\n",
    "3. The gather text chunks are fed to LLM along with original query\n",
    "4. LLM Generates answer to the question\n",
    "\n",
    "Lets take a look at this in action.\n",
    "#### Initializes a Retriever:\n",
    "\n",
    "To be able to fetch the relevant documents, we initialise a retriever from the previously created `vectorstore`. This retriever is responsible for fetching relevant document chunks based on a given query.\n",
    "\n",
    "The output from retriever is then formatted so that we can pass it to LLM for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ffcac2b61715f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever() #initializes a retriever\n",
    "\n",
    "def format_docs(docs):  \n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)  \n",
    "\n",
    "retrieval_chain = retriever | format_docs # Format docs outputted by retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330ab9195689ec3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generation\n",
    "### Initialise the Large Language Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9238175609b42f5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112dcb47d12fc61",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define a Main chain for RAG:\n",
    "\n",
    "The RAG chain is defined here, integrating the retriever, document formatting function, prompt template, language model, and output parser. This chain outlines the entire process of retrieving context, formatting it, prompting the LM with this context and a question, and parsing the LM's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a0fd4cb6e9dc8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate  \n",
    "from langchain_core.output_parsers import StrOutputParser  \n",
    "from langchain_core.runnables import RunnablePassthrough  \n",
    "\n",
    "PROMPT = \"\"\"  \n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:  \n",
    "\"\"\"  \n",
    "\n",
    "  \n",
    "rag_chain = (  \n",
    "    {\"context\": retrieval_chain , \"question\": RunnablePassthrough()}  \n",
    "    | ChatPromptTemplate.from_template(PROMPT)  \n",
    "    | llm  \n",
    "    | StrOutputParser()  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566d0d9fa33e802",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You are ready with basic RAG pipeline!! \n",
    "\n",
    "Now, If you invoke this LLM chain with question, you will get answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdbf7233086de81",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"Do you offer vegetarian food?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5bdb7686cc9fbb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"What loan do you offer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27967e41551ceed",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3be711d89d18e2c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Visualisation of Retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3908ccebd75c3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, we will try to visualise \n",
    "1. Embedding or Vector Space in 3 dimensions. \n",
    "2. Locate query in the vector space\n",
    "3. How we fetch k nearest neighbours\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e03be4bad3572",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "doc_strings = [doc.page_content for doc in docs]\n",
    "vectors = embeddings.embed_documents(doc_strings)\n",
    "# umap_transformer = umap.UMAP(random_state=0, transform_seed=0).fit(vectors) # For 2 dimensions\n",
    "umap_transformer = umap.UMAP(random_state=0, transform_seed=0, n_components=3).fit(vectors) # For 3 dimensions\n",
    "\n",
    "\n",
    "def umap_embed(vectors, umap_transformer):\n",
    "    umap_embeddings = np.array([umap_transformer.transform([vector])[0] for vector in tqdm(vectors)])\n",
    "    return umap_embeddings\n",
    "\n",
    "global_embeddings = umap_embed(vectors, umap_transformer)\n",
    "\n",
    "global_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d287458a3eaf6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(global_embeddings[:, 0], global_embeddings[:, 1], global_embeddings[:, 2], s=10)\n",
    "ax.set_title('Embeddings')\n",
    "# plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb547475101a16a4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_global_embeddings(query, embeddings, retriever, umap_transformer, embed_function, global_embeddings):\n",
    "    q_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    page_contents = [doc.page_content for doc in docs]\n",
    "    vectors_content_vectors = embeddings.embed_documents(page_contents)\n",
    "\n",
    "    query_embeddings = embed_function([q_embedding], umap_transformer)\n",
    "    retrieved_embeddings = embed_function(vectors_content_vectors, umap_transformer)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(global_embeddings[:, 0], global_embeddings[:, 1], global_embeddings[:, 2], s=10, color='gray')\n",
    "    ax.scatter(query_embeddings[:, 0], query_embeddings[:, 1], query_embeddings[:, 2], s=150, marker='X', color='r')\n",
    "    ax.scatter(retrieved_embeddings[:, 0], retrieved_embeddings[:, 1], retrieved_embeddings[:, 2], s=50, facecolors='none', edgecolors='g')\n",
    "    ax.set_title(f'{query}')\n",
    "    # plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f4b50d9ae8816",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calc_global_embeddings(\"Do you offer vegetarian food?\", embeddings, retriever, umap_transformer, umap_embed, global_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d42c1a5a25fd0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calc_global_embeddings(\"What loan do you offer?\", embeddings, retriever, umap_transformer, umap_embed,\n",
    "                       global_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337685263f35f12d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
